{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdbc5b2b-dc11-4da4-81ef-d6fb2a841cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch   \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision \n",
    "from torchvision import datasets      \n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "from collections import Counter\n",
    "import spacy #NLP library for tokenizing our vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475d1815-ee61-448a-a97c-e1def6bc09f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    # CUDA is a an Nvidia GPU model that lets use do calculations on GPU\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else\n",
    "    \n",
    "    # MPS is similar to Cuda\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else\n",
    "    \n",
    "    # Not optimal (training/testing speed reduced greatly)\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "device = \"gpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6393e1b-3573-4302-b9b3-aeb6a2063cb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40455 Image Captions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SETTING UP DATASET\n",
    "path = \"Flickr8k\"\n",
    "captionFile = path + \"/captions.txt\"\n",
    "data = pd.read_csv(captionFile)\n",
    "\n",
    "print(len(data), \"Image Captions\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce092fb3-72d3-4b2c-9976-2508d96e26be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never\n",
      "gon\n",
      "na\n",
      "give\n",
      "you\n",
      "up\n"
     ]
    }
   ],
   "source": [
    "#Spacy is a NLP library that splits sentences and tokenizes words\n",
    "spacyEnglish = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "example = \"Never gonna give you up\"\n",
    "for token in spacyEnglish.tokenizer(example):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd12f142-b6e7-412f-b417-a1159c695a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will build a lexicon from words that appear in our training\n",
    "# captions enough times (frequency threshold). We will assign a numerical\n",
    "# value to each word in vocab: caption --> numeric tensor\n",
    "\n",
    "class Vocabulary:\n",
    "    \n",
    "    padID = 0\n",
    "    \n",
    "    def __init__(self, frequencyThreshold):\n",
    "        \n",
    "        # Decoder dictionary\n",
    "        # Default tokens for padding, start&end of sentence, and unknown word\n",
    "        self.intToString = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "\n",
    "        # Encoder dictionary\n",
    "        self.stringToInt = {word:num for num,word in self.intToString.items()}\n",
    "\n",
    "        # How often a word needs to appear to be given it's own token\n",
    "        self.frequencyThreshold = frequencyThreshold\n",
    "\n",
    "    # Overload python len() function\n",
    "    def __len__(self):\n",
    "        return len(self.intToString)\n",
    "\n",
    "    # Returns list of tokens (words or subwords) from text\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in spacyEnglish.tokenizer(text)]\n",
    "\n",
    "\n",
    "    # Reads in sentences (captions) and words that appear enough get their own token\n",
    "    # Otherwise they are defaultly assigned <UNK>\n",
    "    def buildVocab(self, sentences):\n",
    "        freq = Counter()\n",
    "        i = 4 # Starts at 4 b/c we already have 4 default tokens\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenize(sentence):\n",
    "                freq[word] += 1\n",
    "\n",
    "                # It has become a common enough word to be a nondefault token\n",
    "                if freq[word] == self.frequencyThreshold:\n",
    "                    self.stringToInt[word] = i\n",
    "                    self.intToString[i] = word\n",
    "                    i += 1\n",
    "\n",
    "    # Takes a new sentence and returns a tensor of eachs of it's tokens numerical representation\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.stringToInt[token] if token in self.stringToInt else self.stringToInt[\"<UNK>\"] for token in tokens]\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a5f5ab1-2da5-4602-b63c-130b8bd005a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'i': 4, 'you': 5, \"'m\": 6, 'never': 7, 'gon': 8, 'na': 9, 'and': 10, 'make': 11, 'tell': 12, 'a': 13}\n",
      "[7, 8, 9, 3, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "lyrics = [\n",
    "    \"We\\'re no strangers to love\",\n",
    "    \"You know the rules and so do I\",\n",
    "    \"A full commitment\\'s what I\\'m thinking of\",\n",
    "    \"You wouldn't get this from any other guy\",\n",
    "    \"I just wanna tell you how I\\'m feeling\",\n",
    "    \"Gotta make you understand\",\n",
    "    \"Never gonna give you up\",\n",
    "    \"Never gonna let you down\",\n",
    "    \"Never gonna run around and desert you\",\n",
    "    \"Never gonna make you cry\",\n",
    "    \"Never gonna say goodbye\",\n",
    "    \"Never gonna tell a lie and hurt you\"\n",
    "]\n",
    "\n",
    "v = Vocabulary(2)\n",
    "v.buildVocab(lyrics)\n",
    "print(v.stringToInt)\n",
    "\n",
    "print(v.numericalize(\"Never gonna give you up\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f83e42c-f6d4-4b53-8dec-4ea5d5627fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit from PyTorch dataset class so that we can pass it into a dataloader for batching \n",
    "class FlickrDataset(Dataset):\n",
    "\n",
    "    # Transform is any preprocessing/normalization function that images need to go through\n",
    "    # before training/testing/eval can start (e.g making all images the same size)\n",
    "    def __init__(self, captionFile, imageFile, split, randomSeed, transform=None, frequencyThreshold=5):\n",
    "\n",
    "\n",
    "        # Validate parameters\n",
    "        self.split = split\n",
    "        assert split in {\"TRAIN\", \"TEST\"}\n",
    "        \n",
    "        self.captionDataframe = pd.read_csv(captionFile)\n",
    "        self.imageFile = imageFile\n",
    "        self.transform = transform\n",
    "        self.captionsPerImage = 5\n",
    "\n",
    "        \n",
    "        \n",
    "        # Create split using the given random seed\n",
    "        # reset_index() unshuffles the indexes after we sample\n",
    "        trainingData = self.captionDataframe.sample(frac = 0.90, random_state=randomSeed).reset_index() \n",
    "        testingData = self.captionDataframe.drop(trainingData.index).reset_index()\n",
    "        if split == \"TRAIN\":\n",
    "            self.imageList = trainingData[\"image\"]\n",
    "            self.captionList = trainingData[\"caption\"]\n",
    "        elif split == \"TEST\":\n",
    "            self.imageList = testingData[\"image\"]\n",
    "            self.captionList = testingData[\"caption\"]\n",
    "\n",
    "        \n",
    "        # Setup vocabulary tokenizer using all the captions in the dataset\n",
    "        self.vocab = Vocabulary(frequencyThreshold)\n",
    "        self.vocab.buildVocab(self.captionDataframe[\"caption\"].tolist())\n",
    "\n",
    "        # Create list of actual (non-padded) caption lengths (+2 is for the <start> and <end> sentence tokens)\n",
    "        self.captionLengths = [len(caption)+2 for caption in self.captionList]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captionList)\n",
    "    \n",
    "    # Overloads the [] operator.\n",
    "    # dataset[index] returns a PIL image and the tokenized caption tensor\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Get the caption, and load the image as a PIL RBG Image\n",
    "        caption = self.captionList[index]\n",
    "        imageName = self.imageList[index]\n",
    "        image = Image.open(os.path.join(self.imageFile, imageName)).convert(\"RGB\")\n",
    "\n",
    "        # Apply the specified transforms\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get the numerical tokenizec caption\n",
    "        vector = [self.vocab.stringToInt[\"<SOS>\"]] # Start vector token\n",
    "        vector += self.vocab.numericalize(caption)\n",
    "        vector.append(self.vocab.stringToInt[\"<EOS>\"]) # End vector token\n",
    "\n",
    "        # For Bleu evaluation, might be better to return all captions connected to this image in one go. But for now... it don't matter\n",
    "        return image, torch.tensor(vector), self.captionLengths[index]\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08d3b155-0bdc-4cb1-8959-ebd5a72f34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Utility function for displaying images\n",
    "def showImage(image, title=None):\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(image)\n",
    "    if title != None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b0ec5e8-218e-49a3-b34c-001a3d95ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom func we pass to the dataloader to tell it how to batch images\n",
    "# Since caption batches must all be the same size to be tensors,\n",
    "# We pad the ends of the vector based on the largest caption in the batch\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CaptionCollate:\n",
    "    def __init__(self,pad_idx,batch_first=False):\n",
    "            self.pad_idx = pad_idx\n",
    "            self.batch_first = batch_first \n",
    "    \n",
    "    def __call__(self,batch):\n",
    "\n",
    "        # Concatinates together all the image tensors into a iterable?\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch] \n",
    "        imgs = torch.cat(imgs,dim=0)\n",
    "\n",
    "        # Gets all the images captions (item[1]) and passes them through pad_sequence\n",
    "        # to ensure uniform length of vector\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "        \n",
    "        return imgs,targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcab0072-7c34-4add-af1e-b78726667d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP DATASET AND DATALOADER\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKER = 0 #For some reason it doesn't like num_worker > 0...\n",
    "path = \"Flickr8k\"\n",
    "\n",
    "# Transformed applied to images during preprocessing\n",
    "#    1) Resize to 258x258\n",
    "#    2) Take random 256x256 crop --> Maybe better results IDK\n",
    "#    3) Convert from PIL to tensor\n",
    "#    4) Normalize images to the average and std of colors in the training images of the ResNet model\n",
    "transforms = T.Compose([  \n",
    "    T.Resize(258),\n",
    "    T.RandomCrop(256),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Create custom datasets\n",
    "trainingData = FlickrDataset(\n",
    "    captionFile = path + \"/captions.txt\",\n",
    "    imageFile = path + \"/Images\",\n",
    "    transform = transforms,\n",
    "    split = \"TRAIN\",\n",
    "    randomSeed = 42\n",
    ")\n",
    "\n",
    "testingData = FlickrDataset(\n",
    "    captionFile = path + \"/captions.txt\",\n",
    "    imageFile = path + \"/Images\",\n",
    "    transform = transforms,\n",
    "    split = \"TEST\",\n",
    "    randomSeed = 42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create our dataloaders\n",
    "pad_idx = Vocabulary.padID\n",
    "trainingDataloader = DataLoader(\n",
    "    dataset=trainingData,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=True,\n",
    "    collate_fn=CaptionCollate(pad_idx=pad_idx,batch_first=True)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84bfb1ab-c158-4fb0-b31b-7ac226bb23f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#unpacking the batch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m images, captions, lengths \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#showing info of image in single batch\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(BATCH_SIZE):\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "#generating the iterator from the dataloader\n",
    "dataiter = iter(trainingDataloader)\n",
    "\n",
    "#getting the next batch\n",
    "batch = next(dataiter)\n",
    "\n",
    "#unpacking the batch\n",
    "images, captions, lengths = batch\n",
    "\n",
    "#showing info of image in single batch\n",
    "for i in range(BATCH_SIZE):\n",
    "    img,cap,length = images[i],captions[i],lengths[i]\n",
    "    \n",
    "    caption_label = [dataset.vocab.intToString[token] for token in cap.tolist()]\n",
    "    eos_index = caption_label.index('<EOS>')\n",
    "    caption_label = caption_label[1:eos_index]\n",
    "    caption_label = ' '.join(caption_label)                      \n",
    "    showImage(img,caption_label)\n",
    "    print(length)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93f738-81d4-4787-9a90-0b34c5aca851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE ENCODER MODEL\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encodedImageSize=14):\n",
    "        super(Encoder, self).__init__() # Initialize a nn.Module child class\n",
    "        self.encodedImageSize = encodedImageSize # How large our \"encoded\"/\"learned\" image will be\n",
    "\n",
    "\n",
    "        # Pretrained image classifcation library\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "\n",
    "        # We aren't doing classifcation, but we need the models \"knowledge\" on image features\n",
    "        # So to use transfer learning, we need to remove the last two layers of the model\n",
    "        # Because the last two are the ones that do the classifcation (linear & pooling)\n",
    "        layers = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "        \n",
    "        # A final layer to resize the resnets new last (previously 2nd to last) layer\n",
    "        # To the target size of our encodedImage (constructor parameter) that we will then\n",
    "        # pass to the decoder\n",
    "        self.adaptivePool = nn.AdaptiveAvgPool2d((encodedImageSize, encodedImageSize))\n",
    "\n",
    "\n",
    "        self.allowFineTuning()\n",
    "\n",
    "    def allowFineTuning(self, allow=True):\n",
    "\n",
    "        # Weights of pretrained models usually locked, but since we are fine-tuning\n",
    "        # this model to our specific task (transfer learning), we want to unlock them\n",
    "        \n",
    "        # ONLY UNLOCK 2-4 because the first layer of models has typically already\n",
    "        # learned something very important about the image that we don't want to change\n",
    "        for param in self.resnet.parameters()L\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # See Resnet on WhatDidISee doc\n",
    "        for convolutionLayer in list(self.resnet.children())[5:]:\n",
    "            for param in convolutionLayer:\n",
    "                param.requires_grad = allow\n",
    "\n",
    "    \n",
    "    def forward(self, images):\n",
    "\n",
    "        # A batch of image tensors, passed forward through the layers to encode\n",
    "        # \"Forward propogation\"\n",
    "\n",
    "        #https://stackoverflow.com/questions/67087131/what-is-nchw-format\n",
    "        # NCHW (batchSize, 3 (RGB), imageSize, imageSize)\n",
    "\n",
    "        result = self.resnet(images)         # Pass through ResNet model --> (batchSize, 2048, size/32, size/32)\n",
    "        result = self.adaptivePool(result)   # Adaptive layer --> (batchSize, 2028, encodeSize, encodeSize)\n",
    "        result = result.permute(0,2,3,1)     # Rearrange dimensions --> (batchSize, encodeSize, encodeSize, 2048)\n",
    "\n",
    "        #Layers are permuted to work for the attention model\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a563c7-6803-4035-bffc-363a244093d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention network (tells us where in image to look next)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, encoderDim, decoderDim, attentionDim):\n",
    "        \"\"\"\n",
    "        size of encoded image coming in\n",
    "        size of decoder's RNN\n",
    "        size of this network\n",
    "        \"\"\"\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
